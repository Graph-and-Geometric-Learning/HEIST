{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEIST: Hierarchical Embeddings for Integrated Spatial Transcriptomics\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load spatial transcriptomics data (AnnData format)\n",
    "2. Preprocess data and build hierarchical graphs\n",
    "3. Generate cell representations using a pre-trained model\n",
    "4. Visualize embeddings with PHATE colored by cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import scanpy as sc\n",
    "from utils.preprocess import preprocess\n",
    "from utils.dataloader import create_dataloader\n",
    "from model.model import GraphEncoder\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import phate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Update these paths before running:**\n",
    "- `data_path`: Path to your AnnData file (.h5ad)\n",
    "- `model_path`: Path to pre-trained model checkpoint (.pth)\n",
    "\n",
    "**Required data structure:**\n",
    "- `adata.X`: gene expression matrix\n",
    "- `adata.obsm['spatial']`: spatial coordinates (required)\n",
    "- `adata.obs['cell_type']`: cell type labels (optional, will cluster if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths\n",
    "data_path = \"path/to/adata\"\n",
    "model_path = \"path/to/model\"\n",
    "\n",
    "# Save locations\n",
    "save_root = \"data/preprocessed\"\n",
    "save_file_name = \"sample_data\"\n",
    "\n",
    "# Device configuration (automatically uses GPU if available)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load AnnData file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from {data_path}\")\n",
    "adata = sc.read_h5ad(data_path)\n",
    "print(f\"Loaded AnnData with {adata.n_obs} cells and {adata.n_vars} genes\")\n",
    "print(f\"Available obsm keys: {list(adata.obsm.keys())}\")\n",
    "print(f\"Available obs columns: {list(adata.obs.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess data and create graphs\n",
    "\n",
    "This step will:\n",
    "1. Filter and normalize gene expression\n",
    "2. Select top 200 highly variable genes\n",
    "3. Build spatial cell graph using Voronoi tessellation\n",
    "4. Create gene regulatory networks (GRNs) for each cell type using mutual information\n",
    "5. Save graphs to `{save_root}/{save_file_name}.pt`\n",
    "\n",
    "**Note:** If the .pt file already exists, it will load from cache instead of reprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing data and creating graphs...\")\n",
    "graphs = preprocess(\n",
    "    adata=adata,\n",
    "    save_root=save_root,\n",
    "    save_file_name=save_file_name,\n",
    "    max_genes=200,  # Number of highly variable genes to select\n",
    "    spatial='spatial',  # Key in adata.obsm for spatial coordinates\n",
    "    cell_type='cell_type'  # Set to 'cell_type' if already annotated, None for automatic clustering\n",
    ")\n",
    "print(f\"Created {len(graphs)} graphs:\")\n",
    "print(f\"  - 1 high-level cell graph with {graphs[0].num_nodes} nodes\")\n",
    "print(f\"  - {len(graphs)-1} low-level gene graphs (one per cell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load pre-trained model\n",
    "\n",
    "The checkpoint should contain:\n",
    "- `args`: hyperparameters (pe_dim, hidden_dim, etc.)\n",
    "- `model_state_dict`: trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model from {model_path}\")\n",
    "checkpoint = torch.load(model_path, weights_only = False, map_location=device)\n",
    "args = checkpoint['args']\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  - PE dimension: {args.pe_dim}\")\n",
    "print(f\"  - Hidden dimension: {args.hidden_dim}\")\n",
    "print(f\"  - Output dimension: {args.output_dim}\")\n",
    "# Initialize model architecture\n",
    "model = GraphEncoder(\n",
    "    args.pe_dim, \n",
    "    args.init_dim, \n",
    "    args.hidden_dim, \n",
    "    args.output_dim, \n",
    "    args.num_layers, \n",
    "    args.num_heads, \n",
    "    args.cross_message_passing, \n",
    "    args.pe, \n",
    "    args.blending\n",
    ").to(device)\n",
    "\n",
    "# Load model weights and set to evaluation mode\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate cell representations\n",
    "\n",
    "This uses the pre-trained model to encode both:\n",
    "- **High-level embeddings**: from the spatial cell graph\n",
    "- **Low-level embeddings**: from gene regulatory networks\n",
    "\n",
    "The final representation concatenates both embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Calculating cell representations...\")\n",
    "# batch_size = 128 # Process cells in batches to fit GPU memory\n",
    "# dataloader = create_dataloader(graphs, batch_size, permute=False)\n",
    "\n",
    "# Initialize tensor to store embeddings (2x output_dim for concatenated embeddings)\n",
    "graph_embeddings = torch.zeros((graphs[0].num_nodes, 2*args.output_dim)).to(device)\n",
    "high_level_graph = graphs[0].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for high_level_subgraph, low_level_batch, batch_idx in tqdm(dataloader, desc=\"Encoding cells\"):\n",
    "        high_level_subgraph = high_level_subgraph.to(device)\n",
    "        low_level_batch = low_level_batch.to(device)\n",
    "        low_level_batch.batch_idx = batch_idx.to(device)\n",
    "        \n",
    "        # Encode both high-level (spatial) and low-level (gene) graphs\n",
    "        high_emb, low_emb = model.encode(high_level_subgraph, low_level_batch, args.pe_dim)\n",
    "        \n",
    "        # Combine embeddings: [high-level spatial | low-level gene]\n",
    "        graph_embeddings[batch_idx] = torch.cat([high_emb, global_mean_pool(low_emb, low_level_batch.batch)], dim=1)\n",
    "\n",
    "# Store embeddings in the graph\n",
    "high_level_graph.X = graph_embeddings\n",
    "print(f\"Cell representations calculated: {graph_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save results\n",
    "\n",
    "Save the graph with embeddings for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"data/{save_file_name}_with_embeddings.pt\"\n",
    "torch.save(high_level_graph, output_path)\n",
    "print(f\"Saved graph with embeddings to {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pipeline completed successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Summary:\")\n",
    "print(f\"  - Processed {graphs[0].num_nodes} cells\")\n",
    "print(f\"  - Embedding dimension: {graph_embeddings.shape[1]}\")\n",
    "print(f\"  - Outputs saved:\")\n",
    "print(f\"    â€¢ Graph: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCGFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
